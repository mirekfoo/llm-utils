{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"api/","title":"API Reference","text":"Page Info * llm_utils.causalAttn Causal-attention mechanism classes. * llm_utils.dataLoader Pytorch DataLoader for LLM model. * llm_utils.selfAttn Self-attention mechanism classes."},{"location":"api/llm_utils/causalAttn/","title":"llm_utils.causalAttn","text":""},{"location":"api/llm_utils/causalAttn/#llm_utils.causalAttn","title":"llm_utils.causalAttn","text":"<p>Causal-attention mechanism classes. upon: \"Build a Large Language Model (From Scratch)\" by Sebastian Raschka, chapter 3.5</p>"},{"location":"api/llm_utils/causalAttn/#llm_utils.causalAttn.CausalAttention_V0","title":"CausalAttention_V0","text":"<p>               Bases: <code>Module</code></p> <p>A simple self-attention mechanism implementation using linear layers for query, key, and value projections.</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>The input dimension size.</p> required <code>d_out</code> <code>int</code> <p>The output dimension size for the queries, keys, and values.</p> required <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the linear projections. Defaults to False.</p> <code>False</code>"},{"location":"api/llm_utils/causalAttn/#llm_utils.causalAttn.CausalAttention_V0.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass for the CausalAttention_V0 module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_in).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, d_out).</p>"},{"location":"api/llm_utils/causalAttn/#llm_utils.causalAttn.CausalAttention_V1","title":"CausalAttention_V1","text":"<p>               Bases: <code>Module</code></p> <p>A simple self-attention mechanism implementation using linear layers for query, key, and value projections.</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>The input dimension size.</p> required <code>d_out</code> <code>int</code> <p>The output dimension size for the queries, keys, and values.</p> required <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the linear projections. Defaults to False.</p> <code>False</code>"},{"location":"api/llm_utils/causalAttn/#llm_utils.causalAttn.CausalAttention_V1.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass for the CausalAttention_V1 module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_in).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, d_out).</p>"},{"location":"api/llm_utils/dataLoader/","title":"llm_utils.dataLoader","text":""},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader","title":"llm_utils.dataLoader","text":"<p>Pytorch DataLoader for LLM model. upon: \"Build a Large Language Model (From Scratch)\" by Sebastian Raschka, chapter 2. Utility routines.</p>"},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader.GPTDatasetV1","title":"GPTDatasetV1","text":"<p>               Bases: <code>Dataset</code></p> <p>A custom dataset class for preparing text data for GPT model training. Args:     txt (str): The input text to be tokenized and split into sequences.     tokenizer (Tokenizer): The tokenizer to convert text into token IDs.     max_length (int): The maximum length of each input sequence.     stride (int): The step size to move the window for creating sequences. Attributes:     tokenizer (Tokenizer): The tokenizer used for encoding the text.     input_ids (List[torch.Tensor]): List of input token ID sequences.     target_ids (List[torch.Tensor]): List of target token ID sequences. Methods:     len(): Returns the number of sequences in the dataset.     getitem(idx): Returns the input and target sequences at the specified index.</p>"},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader.print_data_batch","title":"print_data_batch","text":"<pre><code>print_data_batch(data_batch, tokenizer)\n</code></pre> <p>Prints a batch of data using the provided tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>tuple</code> <p>A tuple containing two elements: - inputs: The input data. - targets: The target data.</p> required <code>tokenizer</code> <p>A tokenizer object used to decode the input and target data.</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader.print_data_set","title":"print_data_set","text":"<pre><code>print_data_set(inputs, targets, tokenizer)\n</code></pre> <p>Prints the input and target data sets in a colorized and formatted manner. Args:     inputs (torch.Tensor): The input tensor data.     targets (torch.Tensor): The target tensor data.     tokenizer (Tokenizer): The tokenizer used to decode and colorize the data. The function performs the following steps: 1. Converts the input and target tensors to lists. 2. Colorizes the input and target data using the tokenizer. 3. Decodes the input data using the tokenizer. 4. Calculates the necessary widths for formatting the output. 5. Prints the input data, colorized input data, and target data in a formatted manner.</p>"},{"location":"api/llm_utils/selfAttn/","title":"llm_utils.selfAttn","text":""},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn","title":"llm_utils.selfAttn","text":"<p>Self-attention mechanism classes. upon: \"Build a Large Language Model (From Scratch)\" by Sebastian Raschka, chapter 3.</p>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Linear","title":"SelfAttention_Linear","text":"<p>               Bases: <code>Module</code></p> <p>A simple self-attention mechanism implementation using linear layers for query, key, and value projections.</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>The input dimension size.</p> required <code>d_out</code> <code>int</code> <p>The output dimension size for the queries, keys, and values.</p> required <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the linear projections. Defaults to False.</p> <code>False</code>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Linear.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass for the SelfAttention_Linear module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_in).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, d_out).</p>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Linear4","title":"SelfAttention_Linear4","text":"<p>               Bases: <code>Module</code></p>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Linear4.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass for the SelfAttention_Linear4 module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_in).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, d_out).</p>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Linear5","title":"SelfAttention_Linear5","text":"<p>               Bases: <code>Module</code></p> <p>A simple self-attention mechanism implementation using linear layers for query, key, and value projections.</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>The input dimension size.</p> required <code>d_out</code> <code>int</code> <p>The output dimension size for the queries, keys, and values.</p> required <code>qkv_bias</code> <code>bool</code> <p>If True, adds a learnable bias to the linear projections. Defaults to False.</p> <code>False</code>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Linear5.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass for the SelfAttention_Linear5 module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_in).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, d_out).</p>"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Params","title":"SelfAttention_Params","text":"<p>               Bases: <code>Module</code></p> <p>A simple self-attention mechanism implementation using learnable parameters for query, key, and value projections.</p> <p>Parameters:</p> Name Type Description Default <code>d_in</code> <code>int</code> <p>The input dimension size.</p> required <code>d_out</code> <code>int</code> <p>The output dimension size for the queries, keys, and values.</p> required"},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn.SelfAttention_Params.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass for the SelfAttention_Params module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, d_in).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, d_out).</p>"}]}