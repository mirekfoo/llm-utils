{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"api/","title":"API Reference","text":"Page Info * llm_utils.causalAttn Causal-attention mechanism classes. * llm_utils.dataLoader Pytorch DataLoader for LLM model. * llm_utils.selfAttn Self-attention mechanism classes."},{"location":"api/llm_utils/causalAttn/","title":"llm_utils.causalAttn","text":""},{"location":"api/llm_utils/causalAttn/#llm_utils.causalAttn","title":"llm_utils.causalAttn","text":"<p>Causal-attention mechanism classes. upon: \"Build a Large Language Model (From Scratch)\" by Sebastian Raschka, chapter 3.5</p>"},{"location":"api/llm_utils/dataLoader/","title":"llm_utils.dataLoader","text":""},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader","title":"llm_utils.dataLoader","text":"<p>Pytorch DataLoader for LLM model. upon: \"Build a Large Language Model (From Scratch)\" by Sebastian Raschka, chapter 2. Utility routines.</p>"},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader.GPTDatasetV1","title":"GPTDatasetV1","text":"<p>               Bases: <code>Dataset</code></p> <p>A custom dataset class for preparing text data for GPT model training. Args:     txt (str): The input text to be tokenized and split into sequences.     tokenizer (Tokenizer): The tokenizer to convert text into token IDs.     max_length (int): The maximum length of each input sequence.     stride (int): The step size to move the window for creating sequences. Attributes:     tokenizer (Tokenizer): The tokenizer used for encoding the text.     input_ids (List[torch.Tensor]): List of input token ID sequences.     target_ids (List[torch.Tensor]): List of target token ID sequences. Methods:     len(): Returns the number of sequences in the dataset.     getitem(idx): Returns the input and target sequences at the specified index.</p>"},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader.print_data_batch","title":"print_data_batch","text":"<pre><code>print_data_batch(data_batch, tokenizer)\n</code></pre> <p>Prints a batch of data using the provided tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>tuple</code> <p>A tuple containing two elements: - inputs: The input data. - targets: The target data.</p> required <code>tokenizer</code> <p>A tokenizer object used to decode the input and target data.</p> required <p>Returns:</p> Type Description <p>None</p>"},{"location":"api/llm_utils/dataLoader/#llm_utils.dataLoader.print_data_set","title":"print_data_set","text":"<pre><code>print_data_set(inputs, targets, tokenizer)\n</code></pre> <p>Prints the input and target data sets in a colorized and formatted manner. Args:     inputs (torch.Tensor): The input tensor data.     targets (torch.Tensor): The target tensor data.     tokenizer (Tokenizer): The tokenizer used to decode and colorize the data. The function performs the following steps: 1. Converts the input and target tensors to lists. 2. Colorizes the input and target data using the tokenizer. 3. Decodes the input data using the tokenizer. 4. Calculates the necessary widths for formatting the output. 5. Prints the input data, colorized input data, and target data in a formatted manner.</p>"},{"location":"api/llm_utils/selfAttn/","title":"llm_utils.selfAttn","text":""},{"location":"api/llm_utils/selfAttn/#llm_utils.selfAttn","title":"llm_utils.selfAttn","text":"<p>Self-attention mechanism classes. upon: \"Build a Large Language Model (From Scratch)\" by Sebastian Raschka, chapter 3.</p>"}]}